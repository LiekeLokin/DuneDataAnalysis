{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29b9eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from duneAnalysis import duneAnalysis\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb49c6",
   "metadata": {},
   "source": [
    "# Dune Analysis using the wavelet method\n",
    "\n",
    "## Requirements and principles wavelet method\n",
    "- Wavelet analysis performed over the whole signal, unfitlered, only NaNs should be replaced by values. Either by interploation (in the midddle of the signal) or by extrapolation/duplication of last not NaN value (at the edges of the signal)\n",
    "- The wavelet spectrum (class wavelet) can be saved if this is needed for further analysis\n",
    "- Reconstruct the dunes based on the length scales found in the wavelet spectrum, for the Waal system this the range of scales to create the reconstruction is between 20 m and 300 m. Ripples in the Waal have a length of 5 to 10 m, so the smallest scale of 20 meter ensures ripples will not be recontstructed. Based on a study of the wavelet spectra for different flow regimes the maximum scale should be 300 m. The scales are determined based on the wavelet spectra and a short study of the dune profiles. The scales can be changed to different values for different systems.\n",
    "- Determine the crest locations in the reconstructed profiles.\n",
    "- When studying dunes filter the original signal with a Savitski-Golay filter with lengththscale about 4 times the expected lengthscales of teh ripples in the bed profile. And polynomial 3. Such that the ripples are filtered out and the location of the crest of the primary dune is not altered too much.\n",
    "- To determine the crest locations, correlate the crest locations found in the wavelet-reconstructed profile with the crest locations (local maxima) in the filtered-original profile. The location the local maximum in the origninal-filterd profile closest to each peak in the wavelet filtered signal is then the final crest location.\n",
    "- Throughs are points with the lowest eleveation between two crests.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41607f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data folder (MiddenWaal, or TielstAndries, or LowerWaal)\n",
    "dataFolder = r'Example_data'\n",
    "\n",
    "# read the data from example_data\n",
    "\n",
    "ZFrame = pd.read_csv(os.path.join(dataFolder,'Z_locations.csv')\n",
    "                     ,index_col=0,skiprows = 6)\n",
    "MXYFrame = pd.read_csv(os.path.join(dataFolder,'MXY_locations.csv')\n",
    "                       ,index_col=0,skiprows = 6)\n",
    "DatesFrame = pd.read_csv(os.path.join(dataFolder,'dates.csv')\n",
    "                         ,index_col=0,skiprows = 6)\n",
    "DatesFrame = pd.read_pickle(os.path.join(dataFolder,'dateTable_2011-2021.pkl'))\n",
    "\n",
    "Z = ZFrame.values\n",
    "M = MXYFrame.M.values\n",
    "\n",
    "Zfilt = savgol_filter(Z,41,3,mode='mirror',axis = 0)\n",
    "\n",
    "# define the output folder for the wavelet output\n",
    "outFolder = os.path.join(dataFolder,f'duneParam')\n",
    "if not os.path.exists(outFolder):\n",
    "    os.mkdir(outFolder)\n",
    "\n",
    "# define the maximum and minimum wavelengths that may be considered dunes, for the filtering of the wavelet\n",
    "# lower limit: to exclude ripples, upper limit based on the expected dunes \n",
    "# eg. Wilbers and Ten brinke, 2003; Frings and Kleinhans, 2008; de Ruijsscher et al., 2020; Zomer et al., 2021\n",
    "upperDuneLimit = 300\n",
    "lowerDuneLimit = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2252da2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LokinLR\\Anaconda3\\envs\\duneAnalysis\\lib\\site-packages\\wavelets\\transform.py:104: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  wavelet_data[slices],\n"
     ]
    }
   ],
   "source": [
    "Zreconstructed = np.zeros(np.shape(Z))\n",
    "dnums = np.arange(np.shape(Z)[1])\n",
    "\n",
    "CrestTroughs = pd.DataFrame(columns = ['Crests','Troughs'], index = dnums)\n",
    "  \n",
    "for d in dnums:\n",
    "    Mcrests,Mtroughs,Zreconstructed[:,d],_ = duneAnalysis.Wavelet2Dunes(Z[:,d],\n",
    "                                                             Zfilt[:,d],\n",
    "                                                             M = M[:], \n",
    "                                                             lowLim = lowerDuneLimit, \n",
    "                                                             upLim = upperDuneLimit)\n",
    "    CrestTroughs.loc[d,'Crests']   = [Mcrests]\n",
    "    CrestTroughs.loc[d,'Troughs']  = [Mtroughs]\n",
    "\n",
    "# # save the locations of the crests and trhoughs in a pickle\n",
    "# CrestTroughs.to_pickle(os.path.join(outFolder,'CrestTroughLocations_2011-2021_centerline.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "446b54f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for the dune shape parameters\n",
    "DuneParam = pd.DataFrame(columns = ['Lengths','Heights','LHratios','Slee'], index = dnums)\n",
    "DuneParamStat = pd.DataFrame(columns = ['L','L2','L05','L25','L75','L95',\n",
    "                                        'H','H05','H25','H75','H95',\n",
    "                                        'LH','LH05','LH25','LH75','LH95',\n",
    "                                        'Slee','Slee05','Slee25','Slee75','Slee95'],index = dnums)\n",
    "CrestTroughs.Crests.loc[0][0]\n",
    "\n",
    "frameSize  = np.shape(M)[0] # if one wants to analyse different parts of the data\n",
    "frameShift = 1000\n",
    "\n",
    "for d in dnums:\n",
    "    crests = CrestTroughs.Crests.loc[(d)][0]\n",
    "    troughs = CrestTroughs.Troughs.loc[(d)][0]\n",
    "    # at least 1 complete dune must be present in the signal\n",
    "    if len(crests) < 2:\n",
    "        continue\n",
    "    if len(troughs) < 2:\n",
    "        continue\n",
    "\n",
    "    # determine the dune parameters\n",
    "    lengths,heights,lhratio,Slee = duneAnalysis.duneHeightLengthRatio(Zfilt[:,d], M, crests, troughs)\n",
    "\n",
    "    # store the parameters in the predefined dataframes\n",
    "    DuneParam.loc[(d),'Lengths']  = lengths\n",
    "    DuneParam.loc[(d),'Heights']  = heights\n",
    "    DuneParam.loc[(d),'LHratios'] = lhratio\n",
    "    DuneParam.loc[(d),'Slee']     = Slee\n",
    "\n",
    "    DuneParamStat.loc[(d),'L'] = np.mean(lengths)\n",
    "    DuneParamStat.loc[(d),'H'] = np.mean(heights)\n",
    "    DuneParamStat.loc[(d),'LH'] = np.mean(lhratio)\n",
    "    DuneParamStat.loc[(d),'Slee'] = np.mean(Slee)\n",
    "\n",
    "    DuneParamStat.loc[(d),['L05','L25','L75','L95']] = np.percentile(lengths,[5,25,75,95])\n",
    "    DuneParamStat.loc[(d),['H05','H25','H75','H95']] = np.percentile(heights,[5,25,75,95])\n",
    "    DuneParamStat.loc[(d),['LH05','LH25','LH75','LH95']] = np.percentile(lhratio,[5,25,75,95])\n",
    "    DuneParamStat.loc[(d),['Slee05','Slee25','Slee75','Slee95']] = np.percentile(Slee,[5,25,75,95])\n",
    "# save the parameters in a pickle\n",
    "DuneParam.to_pickle(os.path.join(outFolder,f'DuneParam_all_2011-2021_centerline.pkl')) #{istart:05d}_{iend:05d}\n",
    "DuneParamStat.to_pickle(os.path.join(outFolder,f'DuneParamStat_all_2011-2021_centerline.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8444e26",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'float' and 'Timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7512/3920812105.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mduneSpeedCorr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshift\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshiftdates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mshiftdates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'Timestamp'"
     ]
    }
   ],
   "source": [
    "frameSize  = 1000 #Perform the analysis over sections of 1000 m long\n",
    "frameShift = 1000\n",
    "shifts = np.arange((np.shape(M)[0]+1-frameSize)//frameShift)\n",
    "\n",
    "files = ZFrame.columns.values\n",
    "\n",
    "# create the signal without the underlying bed 1001 may be chosen differently if the underlying bed is more variable\n",
    "Zdunes = Zfilt - savgol_filter(Z,1001,3,mode='mirror',axis = 0)\n",
    "\n",
    "# define dataframes to put data in\n",
    "duneSpeeds       = pd.DataFrame(columns = files, index = shifts) # displacement/dt\n",
    "duneDisplacement = pd.DataFrame(columns = files, index = shifts) # calculated displacements\n",
    "duneSpeedCorr    = pd.DataFrame(columns = files, index = shifts) # correlation at the displacement\n",
    "duneSpeedShiftDates = pd.DataFrame(columns = files, index = shifts)  # dt\n",
    "\n",
    "\n",
    "## TODO Lieke: uitschrijven hoe het precies werkt en dan de code nogmaals controleren\n",
    "for shift in shifts:\n",
    "    shiftStart = int(shift*frameShift)\n",
    "    shiftEnd = int(shift*frameShift+frameSize)\n",
    "    shiftdates = DatesFrame.iloc[shiftStart:shiftEnd].mode().values[0]\n",
    "    \n",
    "    duneSpeedShiftDates.iloc[shift] = shiftdates\n",
    "          \n",
    "    for f,file in enumerate(files[:-1]):\n",
    "        # dune speed by determining the lag in the s\n",
    "        if shiftdates[f] == 0:#datetime.datetime(2000,1,1):\n",
    "            displ,corr = duneAnalysis.lag_finder(Zdunes[shiftStart:shiftEnd,f-1],\n",
    "                                                 Zdunes[shiftStart:shiftEnd,f+1],\n",
    "                                                 1, thresholdMin = 10)\n",
    "            \n",
    "            duneDisplacement.loc[(shift),files[f+1]] = displ\n",
    "            duneSpeedCorr.loc[(shift),files[f+1]] = corr\n",
    "            \n",
    "            duneDisplacement.loc[(shift),files[f]] = np.nan\n",
    "            duneSpeedCorr.loc[(shift),files[f]] = np.nan\n",
    "\n",
    "            dt = shiftdates[f+1]-shiftdates[f-1]\n",
    "            dt = dt.days\n",
    "            duneSpeeds.loc[(shift),files[f+1]] = duneDisplacement.loc[(shift),files[f+1]]/dt\n",
    "            duneSpeeds.loc[(shift),files[f]]   = np.nan\n",
    "        elif shiftdates[f+1] == 0:\n",
    "            displ,corr = duneAnalysis.lag_finder(Zdunes[shiftStart:shiftEnd,f],\n",
    "                                                 Zdunes[shiftStart:shiftEnd,f+2],\n",
    "                                                 1, thresholdMin = 10)\n",
    "            \n",
    "            duneDisplacement.loc[(shift),files[f]] = displ\n",
    "            duneSpeedCorr.loc[(shift),files[f]] = corr\n",
    "\n",
    "            dt = shiftdates[f+2]-shiftdates[f]\n",
    "            dt = dt.days\n",
    "            duneSpeeds.loc[(shift),files[f+1]] = duneDisplacement.loc[(shift),files[f+1]]/dt\n",
    "            \n",
    "        else:\n",
    "            displ,corr = duneAnalysis.lag_finder(Zdunes[shiftStart:shiftEnd,f],\n",
    "                                                 Zdunes[shiftStart:shiftEnd,f+1],\n",
    "                                                 1, thresholdMin = 10) \n",
    "            duneDisplacement.loc[(shift),files[f+1]] = displ\n",
    "            duneSpeedCorr.loc[(shift),files[f+1]] = corr\n",
    "            \n",
    "            dt = shiftdates[f+1]-shiftdates[f]\n",
    "            dt = dt.days\n",
    "\n",
    "            duneSpeeds.loc[(shift),files[f+1]] = duneDisplacement.loc[(shift),files[f+1]]/dt\n",
    "\n",
    "duneSpeeds.to_pickle(os.path.join(outFolder,f'DuneSpeeds_all_2011-2021_centerline.pkl'))\n",
    "duneSpeedCorr.to_pickle(os.path.join(outFolder,f'duneSpeedCorr_all_2011-2021_centerline.pkl'))\n",
    "duneDisplacement.to_pickle(os.path.join(outFolder,f'duneDisplacement_all_2011-2021_centerline.pkl'))\n",
    "duneSpeedShiftDates.to_pickle(os.path.join(outFolder,f'duneSpeedShiftDates_all_2011-2021_centerline.pkl'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f33dfae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2012-05-14 00:00:00')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shiftdates[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610845d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
