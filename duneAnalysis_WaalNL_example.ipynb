{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from duneAnalysis import duneAnalysis\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1860186",
   "metadata": {},
   "source": [
    "## Import input data and define variables\n",
    "\n",
    "For this example test data is given in the folder 'Example_data'.\n",
    "\n",
    "All output will be stored as pandas dataframes in pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41607f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data folder (MiddenWaal, or TielstAndries, or LowerWaal)\n",
    "dataFolder = r'Data'\n",
    "\n",
    "# define the output folder for the wavelet output\n",
    "outFolder = os.path.join(dataFolder,f'duneParam')\n",
    "if not os.path.exists(outFolder):\n",
    "    os.mkdir(outFolder)\n",
    "\n",
    "# read the data from example_data, convert strings in DatesFrame to datetime values\n",
    "ZFrame = pd.read_csv(os.path.join(dataFolder,'Z_locations.csv')\n",
    "                     ,index_col=0,skiprows = 6)\n",
    "MXYFrame = pd.read_csv(os.path.join(dataFolder,'MXY_locations.csv')\n",
    "                       ,index_col=0,skiprows = 6)\n",
    "DatesFrame = pd.read_csv(os.path.join(dataFolder,'dates.csv')\n",
    "                         ,index_col=0,skiprows = 6)\n",
    "for i,_ in enumerate(DatesFrame.columns):\n",
    "    DatesFrame.iloc[:,i] = pd.to_datetime(DatesFrame.iloc[:,i])\n",
    "\n",
    "# make the values of M and Z a numpy arrays\n",
    "Z = ZFrame.values\n",
    "M = MXYFrame.M.values\n",
    "\n",
    "# define the maximum and minimum wavelengths that may be considered dunes, for the filtering of the wavelet\n",
    "# lower limit: to exclude ripples, upper limit based on the expected dunes \n",
    "# eg. Wilbers and Ten brinke, 2003; Frings and Kleinhans, 2008; de Ruijsscher et al., 2020; Zomer et al., 2021\n",
    "upperDuneLimit = 300\n",
    "lowerDuneLimit = 20\n",
    "\n",
    "# Define the shifts and the files variables for the spatial cross=correlation,\n",
    "# Shifts are used to determine the dune celerity for a 1000 m long section\n",
    "# files are in this example the column names of Zframe and DatesFrame, these columns linkt those varables\n",
    "# to the date of measuring of each point.\n",
    "frameSize  = 1000 #Perform the analysis over sections of 1000 m long\n",
    "frameShift = 1000\n",
    "shifts = np.arange((np.shape(M)[0]+1-frameSize)//frameShift)\n",
    "\n",
    "files = ZFrame.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ceeb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the filtered dune profiles, without the \n",
    "Zfilt = savgol_filter(Z,41,3,mode='mirror',axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb49c6",
   "metadata": {},
   "source": [
    "# Dune Analysis; Dune shape using the wavelet method\n",
    "\n",
    "## Requirements and principles wavelet method\n",
    "- Wavelet analysis performed over the whole signal, unfitlered, only NaNs should be replaced by values. Either by interploation (in the midddle of the signal) or by extrapolation/duplication of last not NaN value (at the edges of the signal)\n",
    "- The wavelet spectrum (class wavelet) can be saved if this is needed for further analysis\n",
    "- Reconstruct the dunes based on the length scales found in the wavelet spectrum, for the Waal system this the range of scales to create the reconstruction is between 20 m and 300 m. Ripples in the Waal have a length of 5 to 10 m, so the smallest scale of 20 meter ensures ripples will not be recontstructed. Based on a study of the wavelet spectra for different flow regimes the maximum scale should be 300 m. The scales are determined based on the wavelet spectra and a short study of the dune profiles. The scales can be changed to different values for different systems.\n",
    "- Determine the crest locations in the reconstructed profiles.\n",
    "- When studying dunes filter the original signal with a Savitski-Golay filter with lengththscale about 4 times the expected lengthscales of teh ripples in the bed profile. And polynomial 3. Such that the ripples are filtered out and the location of the crest of the primary dune is not altered too much.\n",
    "- To determine the crest locations, correlate the crest locations found in the wavelet-reconstructed profile with the crest locations (local maxima) in the filtered-original profile. The location the local maximum in the origninal-filterd profile closest to each peak in the wavelet filtered signal is then the final crest location.\n",
    "- Throughs are points with the lowest eleveation between two crests.\n",
    "\n",
    "\n",
    "## Input\n",
    "\n",
    "- matrix/numpy 2d-array with bed elevation (Z), at each row bed elevation at a location, each column is one measurement (time), or one line in the meausurement\n",
    "- Filtered profile (Zfilt)\n",
    "- Location of the measurements along one line (M), dM must be constant along the line.\n",
    "\n",
    "## Output\n",
    "Crest and Trough locations are given as index values for each column in Z/Zfilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2252da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zreconstructed = np.zeros(np.shape(Z))\n",
    "dnums = np.arange(np.shape(Z)[1])\n",
    "\n",
    "CrestTroughs = pd.DataFrame(columns = ['Crests','Troughs'], index = dnums)\n",
    "  \n",
    "for d in dnums:\n",
    "    Mcrests,Mtroughs,Zreconstructed[:,d],_ = duneAnalysis.Wavelet2Dunes(Z[:,d],\n",
    "                                                             Zfilt[:,d],\n",
    "                                                             M = M[:], \n",
    "                                                             lowLim = lowerDuneLimit, \n",
    "                                                             upLim = upperDuneLimit)\n",
    "    CrestTroughs.loc[d,'Crests']   = [Mcrests]\n",
    "    CrestTroughs.loc[d,'Troughs']  = [Mtroughs]\n",
    "\n",
    "# # save the locations of the crests and trhoughs in a pickle\n",
    "# CrestTroughs.to_pickle(os.path.join(outFolder,'CrestTroughLocations_2011-2021_centerline.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7281faf8",
   "metadata": {},
   "source": [
    "## Determine the dune shape parameters\n",
    "The dune shape parameters can be determined for each identified dune (DuneParam) and as statistics over all dunes in one profile, column in Z, in DuneParamStat. The following dune parameters are determined\n",
    "\n",
    "- Dune length, L, horizontal distance between two consequtive troughs in m.\n",
    "- Dune height, H, vertical distancs between a crest and its downstream trough in m.\n",
    "- Aspect ratio, LH, dune height/dune length. Determined for each individual dune and then statistics are determined in m/m.\n",
    "- Lee slope angle, Slee, the mean angle of the middle 4/6th section of the lee slope, in degrees.\n",
    "\n",
    "## Input\n",
    "- Zfilt,\n",
    "- M,\n",
    "- crest locations,\n",
    "- trough locations\n",
    "\n",
    "## Output\n",
    "- dataframe DuneParam; dune parameters for each individual dune in each profile\n",
    "- dataframe DuneParamStat; statistics of dune parameters in each profile. Mean, 5%, 25%, 75% and 95% bounds of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def29261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for the dune shape parameters\n",
    "DuneParam = pd.DataFrame(columns = ['Lengths','Heights','LHratios','Slee'], index = dnums)\n",
    "DuneParamStat = pd.DataFrame(columns = ['L','L2','L05','L25','L75','L95',\n",
    "                                        'H','H05','H25','H75','H95',\n",
    "                                        'LH','LH05','LH25','LH75','LH95',\n",
    "                                        'Slee','Slee05','Slee25','Slee75','Slee95'],index = dnums)\n",
    "CrestTroughs.Crests.loc[0][0]\n",
    "\n",
    "frameSize  = np.shape(M)[0] # if one wants to analyse different parts of the data\n",
    "frameShift = 1000\n",
    "\n",
    "for d in dnums:\n",
    "    crests = CrestTroughs.Crests.loc[(d)][0]\n",
    "    troughs = CrestTroughs.Troughs.loc[(d)][0]\n",
    "    # at least 1 complete dune must be present in the signal\n",
    "    if len(crests) < 2:\n",
    "        continue\n",
    "    if len(troughs) < 2:\n",
    "        continue\n",
    "\n",
    "    # determine the dune parameters\n",
    "    lengths,heights,lhratio,Slee = duneAnalysis.duneHeightLengthRatio(Zfilt[:,d], M, crests, troughs)\n",
    "\n",
    "    # store the parameters in the predefined dataframes\n",
    "    DuneParam.loc[(d),'Lengths']  = lengths\n",
    "    DuneParam.loc[(d),'Heights']  = heights\n",
    "    DuneParam.loc[(d),'LHratios'] = lhratio\n",
    "    DuneParam.loc[(d),'Slee']     = Slee\n",
    "\n",
    "    DuneParamStat.loc[(d),'L'] = np.mean(lengths)\n",
    "    DuneParamStat.loc[(d),'H'] = np.mean(heights)\n",
    "    DuneParamStat.loc[(d),'LH'] = np.mean(lhratio)\n",
    "    DuneParamStat.loc[(d),'Slee'] = np.mean(Slee)\n",
    "\n",
    "    DuneParamStat.loc[(d),['L05','L25','L75','L95']] = np.percentile(lengths,[5,25,75,95])\n",
    "    DuneParamStat.loc[(d),['H05','H25','H75','H95']] = np.percentile(heights,[5,25,75,95])\n",
    "    DuneParamStat.loc[(d),['LH05','LH25','LH75','LH95']] = np.percentile(lhratio,[5,25,75,95])\n",
    "    DuneParamStat.loc[(d),['Slee05','Slee25','Slee75','Slee95']] = np.percentile(Slee,[5,25,75,95])\n",
    "# save the parameters in a pickle\n",
    "DuneParam.to_pickle(os.path.join(outFolder,f'DuneParam_all_2011-2021_centerline.pkl')) #{istart:05d}_{iend:05d}\n",
    "DuneParamStat.to_pickle(os.path.join(outFolder,f'DuneParamStat_all_2011-2021_centerline.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2c251e",
   "metadata": {},
   "source": [
    "# Dune Analysis; celerity using spatial cross-correlation\n",
    "The spatial cross correlation function (lag_finder), determines the the displacement of the profile based on the cross correlation values of one profile moving over the previous profile.\n",
    "\n",
    "- Remove large scale morhpology (point bars, larger bar structures, erosion holes/sedimentation pits linked to river geometry at a larger scale than the dunes)\n",
    "- Make dure the time dune haven't changed too much in shape or have moved more than one dune length between two consequtive profiles; This may be the case if the time interval is too large or during floodwaves when the dunes move fast\n",
    "\n",
    "To determine dune celerity, the displacement is divided over the numer of days between the two measurements.\n",
    "\n",
    "## Input\n",
    "- Zfilt\n",
    "- Z\n",
    "- shifts; number of shifts, frameShift: step for each frame. frameSize: length of each frame\n",
    "- DatesFrame; date of measurement for each value in Z.\n",
    "\n",
    "## Output\n",
    "4 dataframes:\n",
    "- duneSpeeds: dune celerity in each frame per measurement\n",
    "- duneDisplacement: displacement of the profile in each frame per measurement \n",
    "- duneSpeedCorr: correlation factor found for the displacements\n",
    "- duneSpeedShiftDates: dates of measurements for each frame per measurement, determinde as the date that occured most often in that frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147787c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the signal without the underlying bed 1001 may be chosen differently if the underlying bed is more variable\n",
    "Zdunes = Zfilt - savgol_filter(Z,1001,3,mode='mirror',axis = 0)\n",
    "\n",
    "# define dataframes to put data in\n",
    "duneSpeeds       = pd.DataFrame(columns = files, index = shifts) # displacement/dt\n",
    "duneDisplacement = pd.DataFrame(columns = files, index = shifts) # calculated displacements\n",
    "duneSpeedCorr    = pd.DataFrame(columns = files, index = shifts) # correlation at the displacement\n",
    "duneSpeedShiftDates = pd.DataFrame(columns = files, index = shifts)  # dt\n",
    "\n",
    "noval = datetime.datetime.strptime('2000-01-01', '%Y-%m-%d')\n",
    "\n",
    "for shift in shifts:\n",
    "    shiftStart = int(shift*frameShift)\n",
    "    shiftEnd = int(shift*frameShift+frameSize)\n",
    "    shiftdates = DatesFrame.iloc[shiftStart:shiftEnd].mode().values[0]\n",
    "    \n",
    "    duneSpeedShiftDates.iloc[shift] = shiftdates\n",
    "          \n",
    "    for f,file in enumerate(files[:-1]):\n",
    "        # dune speed by determining the spatial lag based in spatial cross-correlation\n",
    "        if (shiftdates[f]!= noval) & (shiftdates[f+1]!= noval):\n",
    "            # normal routine, assign displacement, correlation to date f+1\n",
    "            displ,corr = duneAnalysis.lag_finder(Zdunes[shiftStart:shiftEnd,f],\n",
    "                                                 Zdunes[shiftStart:shiftEnd,f+1],\n",
    "                                                 1, thresholdMin = 5) \n",
    "            duneDisplacement.loc[(shift),files[f+1]] = displ\n",
    "            duneSpeedCorr.loc[(shift),files[f+1]] = corr\n",
    "            \n",
    "            dt = shiftdates[f+1]-shiftdates[f]\n",
    "            dt = dt.astype('timedelta64[D]').astype(int)\n",
    "\n",
    "            duneSpeeds.loc[(shift),files[f+1]] = duneDisplacement.loc[(shift),files[f+1]]/dt\n",
    "        elif (shiftdates[f] == noval) & (shiftdates[f+1] != noval):\n",
    "            displ,corr = duneAnalysis.lag_finder(Zdunes[shiftStart:shiftEnd,f-1],\n",
    "                                                 Zdunes[shiftStart:shiftEnd,f+1],\n",
    "                                                 1, thresholdMin = 5)\n",
    "            \n",
    "            duneDisplacement.loc[(shift),files[f+1]] = displ\n",
    "            duneSpeedCorr.loc[(shift),files[f+1]] = corr\n",
    "            \n",
    "            duneDisplacement.loc[(shift),files[f]] = np.nan\n",
    "            duneSpeedCorr.loc[(shift),files[f]] = np.nan\n",
    "\n",
    "            dt = shiftdates[f+1]-shiftdates[f-1]\n",
    "            dt = dt.astype('timedelta64[D]').astype(int)\n",
    "            duneSpeeds.loc[(shift),files[f+1]] = duneDisplacement.loc[(shift),files[f+1]]/dt\n",
    "            duneSpeeds.loc[(shift),files[f]]   = np.nan\n",
    "        elif (shiftdates[f] != noval) &(shiftdates[f+1] == noval):\n",
    "            # this issue will be solved in case above\n",
    "            continue\n",
    "        elif (shiftdates[f]== noval) & (shiftdates[f+1]== noval):\n",
    "            duneDisplacement.loc[(shift),files[f]] = np.nan\n",
    "            duneSpeedCorr.loc[(shift),files[f]] = np.nan\n",
    "            duneSpeeds.loc[(shift),files[f]]   = np.nan\n",
    "\n",
    "            duneDisplacement.loc[(shift),files[f+1]] = np.nan\n",
    "            duneSpeedCorr.loc[(shift),files[f+1]] = np.nan\n",
    "            duneSpeeds.loc[(shift),files[f+1]]   = np.nan\n",
    "\n",
    "# replace the 0 values in the column without \n",
    "duneSpeedShiftDates = duneSpeedShiftDates.replace(to_replace=0, method='bfill')\n",
    "\n",
    "duneSpeeds.to_pickle(os.path.join(outFolder,f'DuneSpeeds_all_2011-2021_centerline.pkl'))\n",
    "duneSpeedCorr.to_pickle(os.path.join(outFolder,f'duneSpeedCorr_all_2011-2021_centerline.pkl'))\n",
    "duneDisplacement.to_pickle(os.path.join(outFolder,f'duneDisplacement_all_2011-2021_centerline.pkl'))\n",
    "duneSpeedShiftDates.to_pickle(os.path.join(outFolder,f'duneSpeedShiftDates_all_2011-2021_centerline.pkl'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f52187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
